# -*- coding: utf-8 -*-
"""Copy of submission_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDMq1SFsOXS64ot7DjXK2Ss6PYCrOPhh

# Proyek Akhir Machine Learning Terapan: Recommendation System
- **Nama:** Taufan Fajarama Putrawansyah R
- **Email:** tfpruslanali@gmail.com
- **ID Dicoding:** roastland

## Import Semua Packages/Library yang Digunakan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.metrics.pairwise import cosine_similarity

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""## Data Loading and Understanding

Informasi sumber dataset dapat dilihat di tautan berikut
https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset
"""

books = pd.read_csv('https://github.com/roastland/machine-learning-terapan/raw/main/projects/final-project/data/Books.csv')
books.head()

books.info()

"""Berikut informasi umum pada dataset `books`:
- Dataset buku mengidentifikasi buku berdasarkan ISBN. Informasi tambahan meliputi judul buku, pengarang, tahun publikasi, penerbit, dan URL yang mengarah ke gambar sampul buku dari Amazon Web Services. Dataset ini memiliki 271,360 entri buku dengan beberapa kolom memiliki nilai null.
- Ada 271.360 baris (records atau jumlah pengamatan) dalam dataset.

Informasi Kolom:
- `ISBN`: Nomor ISBN buku, bertipe data `object`.
- `Book-Title`: Judul buku, bertipe data `object`.
- `Book-Author`: Pengarang buku, bertipe data `object`.
- `Year-Of-Publication`: Tahun publikasi buku, bertipe data `object`.
- `Publisher`: Penerbit buku, bertipe data `object`.
- `Image-URL-S`, `Image-URL-M`, `Image-URL-L`: URL gambar sampul buku dalam tiga ukuran yang berbeda, bertipe data `object`.
"""

ratings = pd.read_csv('https://github.com/roastland/machine-learning-terapan/raw/main/projects/final-project/data/Ratings.csv')
ratings.head()

ratings.info()

"""Berikut informasi umum pada dataset `ratings`:
- Dataset rating buku berisi informasi tentang rating yang diberikan oleh pengguna pada buku tertentu. Rating dapat bersifat eksplisit, diwakili dalam skala 1-10, atau implisit, diwakili oleh nilai 0 jika tidak ada rating eksplisit yang diberikan. Dataset ini memiliki 1,149,780 entri rating buku yang mencakup kedua jenis rating, eksplisit dan implisit.
- Ada 1.149.780 baris (records atau jumlah pengamatan) dalam dataset.

Informasi Kolom:
- `User-ID`: ID pengguna yang memberikan rating, bertipe data `int64`.
- `ISBN`: Nomor ISBN buku yang diberi rating, bertipe data `object`.
- `Book-Rating`: Rating buku yang diberikan oleh pengguna, bertipe data `int64`.
"""

users = pd.read_csv('https://github.com/roastland/machine-learning-terapan/raw/main/projects/final-project/data/Users.csv')
users.head()

users.info()

"""Berikut informasi umum pada dataset `users`:
- Dataset pengguna mengandung informasi demografis dan anonim dari pengguna Book-Crossing. Beberapa pengguna mungkin memiliki informasi seperti lokasi dan usia. Dataset ini memiliki 278,858 entri pengguna, di mana beberapa pengguna mungkin memiliki informasi usia yang tidak lengkap.
- Ada 278.858 baris (records atau jumlah pengamatan) dalam dataset.

Informasi Kolom:
- `User-ID`: ID pengguna, bertipe data `int64`.
- `Location`: Lokasi pengguna, bertipe data `object`.
- `Age`: Usia pengguna, bertipe data `float64`.

## Exploratory Data Analysis
"""

books.nunique()

ratings.nunique()

users.nunique()

"""Dari 278858 pengguna di dataset `users`, hanya ada 105283 user di dataset `ratings` yang menandakan tidak semua pengguna pernah menilai suatu buku."""

books.describe()

ratings.describe()

users.describe()

"""## Data Preprocessing

Kolom Image akan dihapus karena tidak dibutuhkan. Lalu Kolom Year-Of-Publication akan diubah ke bentuk string karena terdapat nilai yang bukan integer. Hal ini akan dianalisis kemudian.
"""

#copy dataset
df_books = books.copy()

df_books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], inplace=True)

df_books['Year-Of-Publication'] = df_books['Year-Of-Publication'].apply(str)

df_books.sample()

year_freq = df_books['Year-Of-Publication'].value_counts().index.tolist()

year_freq.sort(reverse=True)

print(year_freq)

"""Mengubah data yang salah pada tahun publikasi"""

df_books[df_books['Year-Of-Publication'] == 'DK Publishing Inc']

df_books.loc[df_books['ISBN'] == '078946697X', 'Book-Title'].values[0]

df_books.loc[df_books['ISBN'] == '0789466953', 'Book-Title'].values[0]

df_books.loc[df_books['ISBN'] == '078946697X', 'Publisher'] = "DK Publishing Inc"
df_books.loc[df_books['ISBN'] == '078946697X', 'Year-Of-Publication'] = "2000"
df_books.loc[df_books['ISBN'] == '078946697X', 'Book-Author'] = "Michael Teitelbaum"
df_books.loc[df_books['ISBN'] == '078946697X', 'Book-Title'] = "DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)"

df_books.loc[df_books['ISBN'] == '0789466953', 'Publisher'] = "DK Publishing Inc"
df_books.loc[df_books['ISBN'] == '0789466953', 'Year-Of-Publication'] = "2000"
df_books.loc[df_books['ISBN'] == '0789466953', 'Book-Author'] = "James Buckley"
df_books.loc[df_books['ISBN'] == '0789466953', 'Book-Title'] = "DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)"

df_books.loc[(df_books.ISBN == '078946697X') | (df_books.ISBN == '0789466953')]

df_books[df_books['Year-Of-Publication'] == 'Gallimard']

df_books.loc[df_books['ISBN'] == '2070426769', 'Book-Title'].values[0]

df_books.loc[df_books['ISBN'] == '2070426769', 'Publisher'] = "Gallimard"
df_books.loc[df_books['ISBN'] == '2070426769', 'Year-Of-Publication'] = "2003"
df_books.loc[df_books['ISBN'] == '2070426769', 'Book-Author'] = "Jean-Marie Gustave Le ClÃ?Â©zio"
df_books.loc[df_books['ISBN'] == '2070426769', 'Book-Title'] = "Peuple du ciel, suivi de 'Les Bergers"

df_books.loc[df_books.ISBN == '2070426769']

df_books['Year-Of-Publication'] = df_books['Year-Of-Publication'].apply(int)

print(sorted(df_books['Year-Of-Publication'].unique()))

"""menangani data tahun publikasi yang salah karena seharusnya kolom tahun hanya diisi int, selain itu untuk data tahun bernilai 0 atau di atas 2006, akan diubah dengan median karena bersifat invalid (dataset penelitian dilakukan tahun 2004, 2 tahun tambahan untuk antisipasi jika dataset aslinya terdapat perubahan)"""

df_books.loc[(df_books['Year-Of-Publication'] > 2006) | (df_books['Year-Of-Publication'] == 0), 'Year-Of-Publication'] = np.NAN

df_books.isna().sum()

df_books['Year-Of-Publication'].hist(bins=128, figsize=(16,4))
plt.show()

df_books['Year-Of-Publication'].fillna(round(df_books['Year-Of-Publication'].median()), inplace=True)

"""Data tahun yang invalid akan diganti dengan median."""

df_books['Year-Of-Publication'] = df_books['Year-Of-Publication'].apply(int)
df_books['Year-Of-Publication'] = df_books['Year-Of-Publication'].apply(str)

df_books.isna().sum()

df_books.shape

ratings.isna().sum()

ratings['Book-Rating'].value_counts()

#copy dataset
df_ratings = ratings.copy()

df_ratings.drop(df_ratings.loc[df_ratings['Book-Rating']==0].index, inplace=True)

df_ratings.shape

df_ratings['Book-Rating'].unique()

#copy dataset
df_users = users.copy()

df_users.drop(columns=['Location', 'Age'], inplace=True)

df_users.sample()

"""## Data Preparation

### Preparation for Content Based Fitering
"""

df_merged = pd.merge(df_ratings, df_books, on='ISBN')
df_merged = pd.merge(df_merged, df_users, on='User-ID')

df_merged.head()

df_merged['Book-Features'] = df_merged['Book-Title'] + ', ' + df_merged['Book-Author'] + ', ' + df_merged['Publisher']

df_merged.sample()

df_merged.info()

content_df = df_merged.copy()

content_df.drop(columns=['Book-Author', 'Year-Of-Publication', 'Publisher'], inplace=True)

content_df.shape

content_df = content_df.drop_duplicates('ISBN')

content_df.sort_values('ISBN')

content_df.isna().sum()

content_df.loc[content_df['Book-Features'].isna()]

content_df.dropna(inplace=True)

content_df.isna().sum()

content_df.nunique()

content_df['Book-Features'] = content_df['Book-Features'].apply(lambda x : str(x).lower())
content_df.head()

book_id = content_df['ISBN'].tolist()

book_title = content_df['Book-Title'].tolist()

book_features = content_df['Book-Features'].tolist()

print(len(book_id))
print(len(book_title))
print(len(book_features))

books_dict = pd.DataFrame({
    'id': book_id,
    'book_title': book_title,
    'features': book_features
})
books_dict

"""### Preparation for Collaborative Filtering"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df_ratings['User-ID'].unique().tolist()

# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df_ratings['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

new_ratings = df_ratings.copy()

# Mapping userID ke dataframe user
new_ratings['user'] = new_ratings['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe book
new_ratings['books'] = new_ratings['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_books = len(book_encoded_to_book)
print(num_books)

# Mengubah rating menjadi nilai float
new_ratings['Book-Rating'] = new_ratings['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(new_ratings['Book-Rating'])

# Nilai maksimal rating
max_rating = max(new_ratings['Book-Rating'])

print('Number of User: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

# Mengacak dataset
new_ratings = new_ratings.sample(frac=1, random_state=42)
new_ratings

# Membuat variabel x untuk mencocokkan data user dan books menjadi satu value
x = new_ratings[['user', 'books']].values

# Membuat variabel y untuk membuat rating dari hasil
y = new_ratings['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * new_ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development

### Content Based Filtering
"""

data = books_dict
data.head()

# Reduce size of the data for testing
sample_size = 1000

data_new = data.sample(n=sample_size, random_state=42)

data_new

cv = CountVectorizer(max_features=sample_size, stop_words='english')
cv_matrix = cv.fit_transform(data_new['features'])
cv_matrix.todense()

cv_matrix.shape

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan features
# Baris diisi dengan book_title

pd.DataFrame(
    cv_matrix.todense(),
    columns=data_new.features,
    index=data_new.book_title
).sample(10, axis=1).sample(10, axis=0)

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(cv_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_new['book_title'], columns=data_new['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap books
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=data_new[['book_title', 'features']], k=5):
    """
    Rekomendasi Buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    book_title : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan judul buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar judul buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

title_sample = data_new['book_title'].sample(1).iloc[0]

data_new[data_new.book_title.eq(title_sample)]

book_recommendations(title_sample)

"""### Collaborative Filtering"""

# Reduce size of the data for testing
sample_size = 1000

new_ratings = new_ratings.sample(n=sample_size, random_state=42)

new_ratings

num_users = len(new_ratings['user'].unique().tolist())
num_books = len(new_ratings['books'].unique().tolist())

print(num_users)
print(num_books)

# Membuat variabel x untuk mencocokkan data user dan books menjadi satu value
x = new_ratings[['user', 'books']].values

# Membuat variabel y untuk membuat rating dari hasil
y = new_ratings['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * new_ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.books_embedding = layers.Embedding( # layer embeddings books
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.books_bias = layers.Embedding(num_books, 1) # layer embedding books bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    books_vector = self.books_embedding(inputs[:, 1]) # memanggil layer embedding 3
    books_bias = self.books_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_books, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

new_books = books_dict.copy()
df = new_ratings.copy()

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_rated_by_user = df[df['User-ID'] == user_id]

books_not_read = new_books[~new_books['id'].isin(book_rated_by_user['ISBN'].values)]['id']
books_not_read = list(
    set(books_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)

books_not_read = [[book_to_book_encoded.get(x)] for x in books_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_books_array = np.hstack(
    ([[user_encoder]] * len(books_not_read), books_not_read)
)

ratings = model.predict(user_books_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_books_ids = [
    book_encoded_to_book.get(books_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

top_books_user = (
    book_rated_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    ['ISBN'].values
)

books_df_rows = new_books[new_books['id'].isin(top_books_user)]
for row in books_df_rows.itertuples():
    print(row.book_title, '[', row.features, ']')

print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)

recommended_books = new_books[new_books['id'].isin(recommended_books_ids)]
for row in recommended_books.itertuples():
    print(row.book_title, '[', row.features, ']')

"""Model berhasil memberikan top-N rekomendasi buku kepada pengguna berdasarkan data historisnya."""