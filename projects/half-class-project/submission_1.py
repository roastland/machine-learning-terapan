# -*- coding: utf-8 -*-
"""submission_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JxyBJ-NZt04J8SA-6vFVgTTQQUK87rq7

# Proyek Pertama Machine Learning Terapan: Bike Sharing Dataset
- **Nama:** Taufan Fajarama Putrawansyah R
- **Email:** tfpruslanali@gmail.com
- **ID Dicoding:** roastland

## Import Semua Packages/Library yang Digunakan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from xgboost import XGBRegressor

"""## Data Loading"""

# bike sharing day dataset
url_day = 'https://raw.githubusercontent.com/roastland/machine-learning-terapan/main/projects/half-class-project/Bike-sharing-dataset/day.csv'
bike_day = pd.read_csv(url_day)
bike_day

"""Berikut informasi umum pada dataset `day.csv`:
- Ada 731 baris (records atau jumlah pengamatan) dalam dataset.
- Terdapat 16 kolom yaitu: instant, dteday, season, yr, mnth, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt.
"""

# bike sharing hour dataset
url_hour = 'https://raw.githubusercontent.com/roastland/machine-learning-terapan/main/projects/half-class-project/Bike-sharing-dataset/hour.csv'
bike_hour = pd.read_csv(url_hour)
bike_hour

"""Berikut informasi umum pada dataset `hour.csv`:
- Ada 17.379 baris (records atau jumlah pengamatan) dalam dataset.
- Terdapat 17 kolom yaitu: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt.

## Exploratory Data Analysis

### Deskripsi dan Transformasi Variabel

Informasi Atribut:

`hour.csv` dan `day.csv` sama-sama memiliki kolom berikut, kecuali `hr` tidak ada di `day.csv`

- instant: index data
- dteday : tanggal
- season : musim (1: winter, 2: spring, 3: summer, 4: fall)
- yr : tahun (0: 2011, 1: 2012)
- mnth : bulan (1 sampai 12)
- hr : jam (0 sampai 23)
- holiday : 1 jika hari libur atau 0 jika tidak (diambil dari http://dchr.dc.gov/page/holiday-schedule)
- weekday : hari dalam 1 minggu (0 sampai 6: minggu-sabtu)
- workingday : 1 jika bukan weekend (sabtu/minggu) atau bukan holiday, 0 jika selain itu.
- weathersit : cuaca (1: Clear, 2: Mist, 3: Light Rain/Snow, 4: Heavy Rain/Snow)
- temp : temperatur dalam Celsius yang sudah dinormalisasi. Nilainya didapat dari (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (hanya dalam skala perjam)
- atemp: temperatur yang dirasakan dalam Celsius yang sudah dinormalisasi. Nilainya didapat dari (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (hanya dalam skala perjam)
- hum: kelembaban yang sudah dinormalisasi. Nilainya dibagi dengan 100 (max)
- windspeed: kecepatan angin yang sudah dinormalisasi. Nilainya dibagi dengan 67 (max)
- casual: jumlah pengguna biasa
- registered: jumlah pengguna yang terdaftar
- cnt: jumlah total casual dan registered

Informasi lebih lanjut dapat dilihat di tautan berikut
http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
"""

bike_day.info()

"""Pada `day.csv` terlihat bahwa:

- Terdapat 1 kolom dengan tipe object, yaitu: `dteday`. Kolom ini menandakan time-series data.
- Terdapat 11 kolom dengan tipe data int64. Terdiri dari 1 kolom indeks (`instant`), 5 kolom categorical features yang sudah di-encoding dalam numerik (`season, yr, mnth, weekday, weathersit`) dan binary (`holiday, workingday`), dan 3 kolom target (`casual, registered, cnt`).
- Terdapat 4 kolom dengan tipe data float64. Terdiri dari 4 kolom numerical features (`temp, atemp, hum, windspeed`).
"""

bike_hour.info()

"""Pada `hour.csv` terlihat bahwa:

- Terdapat 1 kolom dengan tipe object, yaitu: `dteday`. Kolom ini menandakan time-series data.
- Terdapat 12 kolom dengan tipe data int64. Terdiri dari 1 kolom indeks (`instant`), 6 kolom categorical features yang sudah di-encoding dalam numerik (`season, yr, mnth, hr, weekday, weathersit`) dan binary (`holiday, workingday`), dan 3 kolom target (`casual, registered, cnt`).
- Terdapat 4 kolom dengan tipe data float64. Terdiri dari 4 kolom numerical features (`temp, atemp, hum, windspeed`).
"""

bike_day.describe()

bike_hour.describe()

"""---


Berdasarkan informasi atribut dan informasi dataset `day.csv` dan `hour.csv`, diputuskan untuk melakukan beberapa transformasi:
- menghapus kolom `instant` karena hanya menunjukkan indeks
- merubah datatype kolom `dteday` menjadi datetime dan menjadikannya indeks dataframe
- merubah categorical features `season`, `weekday`, dan `weathersit` dari bentuk numerikal menjadi ordinal agar lebih deskriptif dan nantinya dapat dilakukan one hot encoding
- menghapus kolom `cnt` sehingga kolom target hanya `casual` dan `registered` karena kolom `cnt` hanya menjumlahkan kedua kolom tersebut

#### `Day` Dataset
"""

# copy dataset
df_day = bike_day.copy()

# menghapus kolom `instant` dan `cnt`
df_day.drop(columns=['instant','cnt'], inplace=True)

# merubah datatype `dteday` dan menjadikannya index
df_day['dteday'] = pd.to_datetime(bike_day['dteday'])
df_day.set_index('dteday', inplace=True)

# merubah `season`, `weekday`, dan `weathersit` dari numerik menjadi ordinal
df_day['season'].replace({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}, inplace=True)
df_day['weekday'].replace({0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday'}, inplace=True)
df_day['weathersit'].replace({1: 'Clear', 2: 'Mist', 3: 'Light Rain/Snow', 4: 'Heavy Rain/Snow'}, inplace=True)

df_day.sample()

df_day.info()

"""Kondisi `day.csv` setelah ditransformasi:

- Indeks data merupakan time-series
- Terdapat 3 kolom dengan tipe object, yaitu: `season`, `weekday`, dan `weathersit`. Kolom ini menandakan categorical features dan akan dilakukan one hot encoding nantinya.
- Terdapat 6 kolom dengan tipe data int64. Terdiri dari 4 kolom categorical features yang sudah di-encoding dalam numerik (`yr, mnth`) dan binary (`holiday, workingday`), dan 2 kolom target (`casual, registered`).
- Terdapat 4 kolom dengan tipe data float64. Terdiri dari 4 kolom numerical features (`temp, atemp, hum, windspeed`).

#### `Hour` Dataset
"""

#copy dataset
df_hour = bike_hour.copy()

# menghapus kolom `instant` dan `cnt`
df_hour.drop(columns=['instant','cnt'], inplace=True)

# merubah datatype `dteday` dan menjadikannya index
df_hour['dteday'] = pd.to_datetime(bike_hour['dteday'])
df_hour.set_index('dteday', inplace=True)

# merubah `season`, `weekday`, dan `weathersit` dari numerik menjadi ordinal
df_hour['season'].replace({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}, inplace=True)
df_hour['weekday'].replace({0: 'Sunday', 1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday'}, inplace=True)
df_hour['weathersit'].replace({1: 'Clear', 2: 'Mist', 3: 'Light Rain/Snow', 4: 'Heavy Rain/Snow'}, inplace=True)

df_hour.sample()

df_hour.info()

"""Kondisi `hour.csv` setelah ditransformasi:

- Indeks data merupakan time-series
- Terdapat 3 kolom dengan tipe object, yaitu: `season`, `weekday`, dan `weathersit`. Kolom ini menandakan categorical features dan akan dilakukan one hot encoding nantinya.
- Terdapat 7 kolom dengan tipe data int64. Terdiri dari 5 kolom categorical features yang sudah di-encoding dalam numerik (`yr, mnth, hr`) dan binary (`holiday, workingday`), dan 2 kolom target (`casual, registered`).
- Terdapat 4 kolom dengan tipe data float64. Terdiri dari 4 kolom numerical features (`temp, atemp, hum, windspeed`).
"""

target_columns = ['casual', 'registered']

numerical_features = ['temp', 'atemp', 'hum', 'windspeed']

day_categorical_features = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']
hour_categorical_features = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']

"""### Menangani Missing Value dan Outliers"""

df_day.isna().sum()

df_hour.isna().sum()

"""Tidak terdapat missing value pada dataset `day` dan dataset `hour`"""

df_day.nunique()

df_hour.nunique()

"""Pada categorical features, kolom `season`, `yr`, `mnth`, `hr`, `holiday`, `weekday`, dan `workingday` memiliki nilai unik yang konsisten untuk kedua dataset. Sedangkan kolom `weathersit` terdapat perbedaan di mana dataset `day` hanya memiliki 3 nilai dan dataset `hour` memiliki 4 nilai."""

# fungsi untuk menampilkan outlier menggunakan boxplot
def plot_multiple_boxplots(df, cols):
    num_cols = len(cols)

    ncols = 2
    nrows = (num_cols + ncols - 1) // ncols

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 5*nrows))
    axes = axes.flatten()

    for i, column in enumerate(cols):
        sns.boxplot(x=df[column], ax=axes[i])
        axes[i].set_title(f'Boxplot of `{column}`')

    for j in range(i+1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

plot_multiple_boxplots(df_day, numerical_features)

plot_multiple_boxplots(df_hour, numerical_features)

"""Pada numerical features, kolom `temp` dan `atemp` tidak terdapat outlier di kedua dataset. Sedangkan kolom `hum` dan `windspeed` terdapat outlier di kedua dataset."""

# fungsi untuk imputasi nilai outlier
def impute_outliers_with_iqr(df, cols):
    new_df = df.copy()

    for column in cols.columns:
        Q1 = new_df[column].quantile(0.25)
        Q3 = new_df[column].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        new_df.loc[new_df[column] < lower_bound, column] = Q1
        new_df.loc[new_df[column] > upper_bound, column] = Q3

    return new_df

"""Kedua dataset merupakan time-series dataset sehingga apabila ada baris yang dihapus akan menghilangkan kontinuitas dari data.

Maka dari itu, outlier di kedua dataset akan dilakukan imputasi nilai menggunakan nilai `lower_bound` atau `upper_bound` sehingga kontinuitas data tetap terjaga dan outlier dapat ditangani.

#### `Day` Dataset
"""

# copy dataset
df_day_imputed = df_day.copy()

# pilih kolom `hum` dan `windspeed`
outlier_cols_day = df_day_imputed[['hum', 'windspeed']]

df_day_imputed = impute_outliers_with_iqr(df_day_imputed, outlier_cols_day)
df_day_imputed.sample()

plot_multiple_boxplots(df_day_imputed, ['hum', 'windspeed'])

"""#### `Hour` Dataset"""

# copy dataset
df_hour_imputed = df_hour.copy()

# pilih kolom `hum` dan `windspeed`
outlier_cols_hour = df_hour_imputed[['hum', 'windspeed']]

df_hour_imputed = impute_outliers_with_iqr(df_hour_imputed, outlier_cols_hour)
df_hour_imputed.sample()

plot_multiple_boxplots(df_hour_imputed, ['hum', 'windspeed'])

"""### Univariate Analysis"""

# fungsi untuk menampilkan barchart dari setiap categorical features
def categorical_feat(df, feature):
    count = df[feature].value_counts()
    percent = 100*df[feature].value_counts(normalize=True)
    info_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
    print(info_df)

    plt.figure(figsize=(8, 4))
    count.plot(kind='bar', title=feature, color=sns.color_palette("Set2", n_colors=len(count)))
    plt.show()

"""#### `Day` Dataset"""

df_day_imputed[numerical_features + target_columns].hist(bins=50, figsize=(24,12))
plt.show()

"""Dari histogram `casual`, beberapa informasi di antaranya:

- Sebagian data terdapat jumlah pengguna harian di bawah 1000.
- Distribusi data miring ke kanan (right-skewed).

Dari histogram `registered`, beberapa informasi di antaranya:

- Distribusi data normal (normal distribution).
"""

for col in day_categorical_features:
  categorical_feat(df_day_imputed, col)

"""Kolom `season`, `yr`, `mnth`, `weekday` terdistribusi merata di setiap nilai karena menandakan indikator hari. Kolom `holiday` dan `workingday` juga bernilai sesuai dengan keadaan indikator hari (lebih banyak workingday dan sedikit holiday). Kolom `weathersit` didominasi kondisi `Clear`, diikuti `Mist`, dan paling sedikit `Light Rain/Snow`.

#### `Hour` Dataset
"""

df_hour_imputed[numerical_features + target_columns].hist(bins=50, figsize=(24,12))
plt.show()

"""Dari histogram `casual`, beberapa informasi di antaranya:

- Sebagian data terdapat jumlah pengguna perjam di bawah 50.
- Distribusi data miring ke kanan (right-skewed).

Dari histogram `registered`, beberapa informasi di antaranya:

- Sebagian data terdapat jumlah pengguna perjam di bawah 200.
- Distribusi data miring ke kanan (right-skewed).
"""

for col in hour_categorical_features:
  categorical_feat(df_hour_imputed, col)

"""Kolom `season`, `yr`, `mnth`, `hr`, `weekday` terdistribusi merata di setiap nilai karena menandakan indikator hari. Kolom `holiday` dan `workingday` juga bernilai sesuai dengan keadaan indikator hari (lebih banyak workingday dan sedikit holiday). Kolom `weathersit` didominasi kondisi `Clear`, diikuti `Mist`, `Light Rain/Snow`, dan paling sedikit `Heavy Rain/Snow`.

### Multivariate Analysis

#### Time Series
"""

plt.figure(figsize=(10, 5))

for column in target_columns:
    plt.plot(df_day_imputed.index, df_day_imputed[column], label=column)

plt.title('Pengguna `casual` dan `registered` Per Hari Seiring Waktu')
plt.xlabel('Date')
plt.ylabel('Count')
plt.legend()

plt.show()

"""Pengguna `casual` dan `registered` menunjukkan kenaikan jumlah di tahun 2012 dibanding 2011."""

# Group setiap jam 'hr' dan hitung rata-rata `casual` dan `registered`
hour = df_hour_imputed.groupby('hr')[target_columns].mean()

plt.figure(figsize=(10, 5))
plt.plot(hour.index, hour['casual'], label='Casual', marker='o')
plt.plot(hour.index, hour['registered'], label='Registered', marker='o')

plt.title('Rata-Rata Pengguna `casual` dan `registered` Per Jam')
plt.xlabel('Hour')
plt.ylabel('Average Count')
plt.legend()

plt.show()

"""Pengguna `casual` cenderung lebih banyak di sekitar jam 12-16, sedangkan pengguna `registered` puncaknya di jam 8 dan jam 17-18. Dapat diasumsikan bahwa pengguna `registered` adalah para pekerja yang berangkat kerja (jam 8) dan pulang kerja (jam 17).

#### Categorical Features

##### `Day` Dataset
"""

for col in day_categorical_features:
  sns.catplot(x=col, y="casual", kind="bar", dodge=False, height = 4, aspect = 3,  data=df_day_imputed, palette="Set3", hue=col, legend=False)
  plt.title("Rata-Rata Pengguna `casual` Per Hari Relatif Terhadap `{}`".format(col))

"""Pada setiap fitur kategorikal, pengguna `casual` per hari cenderung lebih banyak di musim spring/summer, tahun 2012, bulan april-september, hari libur, hari sabtu/minggu, bukan hari kerja, dan pada cuaca cerah. Dapat diasumsikan bahwa pengguna `casual` adalah para pengguna yang sedang berlibur."""

for col in day_categorical_features:
  sns.catplot(x=col, y="registered", kind="bar", dodge=False, height = 4, aspect = 3,  data=df_day_imputed, palette="Set3", hue=col, legend=False)
  plt.title("Rata-rata Pengguna `registered` Per Hari Relatif terhadap `{}`".format(col))

"""Pada setiap fitur kategorikal, pengguna `registered` per hari cenderung lebih banyak di musim spring/summer/fall, tahun 2012, bulan mei-oktober, bukan hari libur, hari senin-jumat, hari kerja, dan pada cuaca cerah/berkabut. Dapat diasumsikan bahwa pengguna `registered` adalah para pengguna yang bekerja kantoran.

##### `Hour` Dataset
"""

for col in hour_categorical_features:
  sns.catplot(x=col, y="casual", kind="bar", dodge=False, height = 4, aspect = 3,  data=df_hour_imputed, palette="Set3", hue=col, legend=False)
  plt.title("Rata-Rata Pengguna `casual` Per Jam Relatif Terhadap `{}`".format(col))

"""Pada setiap fitur kategorikal, pengguna `casual` per jam kurang lebih sama seperti pengguna `casual` per hari, dengan tambahan cenderung lebih banyak di jam 12-17. Hal ini semakin menguatkan asumsi bahwa pengguna `casual` adalah para pengguna yang sedang berlibur."""

for col in hour_categorical_features:
  sns.catplot(x=col, y="registered", kind="bar", dodge=False, height = 4, aspect = 3,  data=df_hour_imputed, palette="Set3", hue=col, legend=False)
  plt.title("Rata-Rata Pengguna `registered` Per Jam Relatif Terhadap `{}`".format(col))

"""Pada setiap fitur kategorikal, pengguna `registered` per jam kurang lebih sama seperti pengguna `registered` per hari, dengan tambahan cenderung lebih banyak di jam 8 dan 17-18. Hal ini semakin menguatkan asumsi bahwa pengguna `registered` adalah para pengguna yang bekerja kantoran.

#### Numerical Features
"""

sns.pairplot(df_day_imputed[numerical_features + target_columns], diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df_day_imputed[numerical_features + target_columns].corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik Dataset `day`", size=20)

"""Pada pola sebaran pairplot dan correlation matrix untuk fitur numerik dataset `day` tidak terdapat fitur yang signifikan berkorelasi (mendekati -1 atau 1) terhadap kolom target (`casual` dan `registered`). Kolom `temp` dan `atemp` memiliki korelasi paling besar dengan nilai korelasi 0.54."""

sns.pairplot(df_hour_imputed[numerical_features + target_columns], diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df_hour_imputed[numerical_features + target_columns].corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik Dataset `hour`", size=20)

"""Pada pola sebaran pairplot dan correlation matrix untuk fitur numerik dataset `hour` tidak terdapat fitur yang signifikan berkorelasi (mendekati -1 atau 1) terhadap kolom target (`casual` dan `registered`). Kolom `temp` memiliki korelasi paling besar dengan nilai korelasi 0.46 dan 0.34.

## Data Preparation

### Encoding Fitur Kategori
"""

day_encoding_cols = ['season', 'mnth', 'weekday', 'weathersit']
hour_encoding_cols = ['season', 'mnth', 'hr', 'weekday', 'weathersit']

encoded_df_day = df_day_imputed.copy()
encoded_df_hour = df_hour_imputed.copy()

for col in day_encoding_cols:
  encoded_df_day = pd.concat([encoded_df_day, pd.get_dummies(encoded_df_day[col], prefix=col, dtype=int)], axis=1)

encoded_df_day.drop(day_encoding_cols, axis=1, inplace=True)

for col in hour_encoding_cols:
  encoded_df_hour = pd.concat([encoded_df_hour, pd.get_dummies(encoded_df_hour[col], prefix=col, dtype=int)], axis=1)

encoded_df_hour.drop(hour_encoding_cols, axis=1, inplace=True)

encoded_df_day.sample()

encoded_df_hour.sample()

"""### Reduksi Dimensi dengan PCA

Pada pola sebaran pairplot dan correlation matrix di section EDA, dapat dilihat bahwa kolom `temp` dan `atemp` berkorelasi tinggi.
"""

sns.pairplot(encoded_df_day[['temp','atemp']], plot_kws={"s": 2});

sns.pairplot(encoded_df_hour[['temp','atemp']], plot_kws={"s": 2});

"""Kolom `temp` dan `atemp` memiliki korelasi yang tinggi satu sama lain yang menunjukkan data yang berulang atau redundant sehingga dapat direduksi dimensinya.

#### `Day` Dataset
"""

pca = PCA(n_components=2, random_state=123)
pca.fit(encoded_df_day[['temp','atemp']])
princ_comp = pca.transform(encoded_df_day[['temp','atemp']])

pca.explained_variance_ratio_.round(3)

df_day_ready = encoded_df_day.copy()

pca = PCA(n_components=1, random_state=123)
pca.fit(df_day_ready[['temp','atemp']])
df_day_ready['temperature'] = pca.transform(df_day_ready.loc[:, ('temp','atemp')]).flatten()
df_day_ready.drop(['temp','atemp'], axis=1, inplace=True)

df_day_ready.sample()

"""#### `Hour` Dataset"""

pca = PCA(n_components=2, random_state=123)
pca.fit(encoded_df_hour[['temp','atemp']])
princ_comp = pca.transform(encoded_df_hour[['temp','atemp']])

pca.explained_variance_ratio_.round(3)

df_hour_ready = encoded_df_hour.copy()

pca = PCA(n_components=1, random_state=123)
pca.fit(df_hour_ready[['temp','atemp']])
df_hour_ready['temperature'] = pca.transform(df_hour_ready.loc[:, ('temp','atemp')]).flatten()
df_hour_ready.drop(['temp','atemp'], axis=1, inplace=True)

df_hour_ready.sample()

"""### Train-Test-Split

train_test_split dataset `day` menggunakan proporsi pembagian data latih dan uji 80:20 karena memiliki jumlah sampel kurang dari 1.000 sampel. Kolom target `casual` dan `registered` dipisah untuk mempermudah analisis prediksi yang modular.
"""

X_day = df_day_ready.drop(target_columns, axis=1)

y_day_cas = df_day_ready["casual"]
X_train_day_c, X_test_day_c, y_train_day_c, y_test_day_c = train_test_split(X_day, y_day_cas, test_size=0.2, random_state=123)

y_day_reg = df_day_ready["registered"]
X_train_day_r, X_test_day_r, y_train_day_r, y_test_day_r = train_test_split(X_day, y_day_reg, test_size=0.2, random_state=123)

print(f'Total # of sample in whole day dataset: {len(X_day)}')
print(f'Total # of sample in train `casual` day dataset: {len(X_train_day_c)}')
print(f'Total # of sample in test `casual` day dataset: {len(X_test_day_c)}')
print(f'Total # of sample in train `registered` day dataset: {len(X_train_day_r)}')
print(f'Total # of sample in test `registered` day dataset: {len(X_test_day_r)}')

"""train_test_split dataset `hour` menggunakan proporsi pembagian data latih dan uji 90:10 karena memiliki jumlah sampel yang banyak (lebih dari 10.000 sampel). Kolom target `casual` dan `registered` dipisah untuk mempermudah analisis prediksi yang modular."""

X_hour = df_hour_ready.drop(target_columns, axis=1)

y_hour_cas = df_hour_ready["casual"]
X_train_hour_c, X_test_hour_c, y_train_hour_c, y_test_hour_c = train_test_split(X_hour, y_hour_cas, test_size=0.1, random_state=123)

y_hour_reg = df_hour_ready["registered"]
X_train_hour_r, X_test_hour_r, y_train_hour_r, y_test_hour_r = train_test_split(X_hour, y_hour_reg, test_size=0.1, random_state=123)

print(f'Total # of sample in whole hour dataset: {len(X_hour)}')
print(f'Total # of sample in train `casual` hour dataset: {len(X_train_hour_c)}')
print(f'Total # of sample in test `casual` hour dataset: {len(X_test_hour_c)}')
print(f'Total # of sample in train `registered` hour dataset: {len(X_train_hour_r)}')
print(f'Total # of sample in test `registered` hour dataset: {len(X_test_hour_r)}')

"""### Standarisasi Fitur Numerik

Standarisasi fitur numerik menggunakan StandardScaler untuk menggeser distribusi data mendekati distribusi normal. Algoritma machine learning memiliki performa yang lebih baik ketika data memiliki skala relatif sama.
"""

numerical_features = ['temperature', 'hum', 'windspeed']

"""#### Dataset `day` - Target `casual`"""

scaler_day_cas = StandardScaler()
scaler_day_cas.fit(X_train_day_c[numerical_features])
X_train_day_c[numerical_features] = scaler_day_cas.transform(X_train_day_c.loc[:, numerical_features])
X_train_day_c[numerical_features].head()

X_train_day_c[numerical_features].describe().round(4)

"""#### Dataset `day` - Target `registered`"""

scaler_day_reg = StandardScaler()
scaler_day_reg.fit(X_train_day_r[numerical_features])
X_train_day_r[numerical_features] = scaler_day_reg.transform(X_train_day_r.loc[:, numerical_features])
X_train_day_r[numerical_features].head()

X_train_day_r[numerical_features].describe().round(4)

"""#### Dataset `hour` - Target `casual`"""

scaler_hour_cas = StandardScaler()
scaler_hour_cas.fit(X_train_hour_c[numerical_features])
X_train_hour_c[numerical_features] = scaler_hour_cas.transform(X_train_hour_c.loc[:, numerical_features])
X_train_hour_c[numerical_features].head()

X_train_hour_c[numerical_features].describe().round(4)

"""#### Dataset `hour` - Target `registered`"""

scaler_hour_reg = StandardScaler()
scaler_hour_reg.fit(X_train_hour_r[numerical_features])
X_train_hour_r[numerical_features] = scaler_hour_reg.transform(X_train_hour_r.loc[:, numerical_features])
X_train_hour_r[numerical_features].head()

X_train_hour_r[numerical_features].describe().round(4)

"""## Model Development

Model yang digunakan akan mengikuti yang diajarkan di kelas (K-Nearest Neighbor, Random Forest, Adaptive Boosting) dengan tambahan 1 algoritma gradient boosting, yaitu algoritma XGBoost (eXtreme Gradient Boosting).
"""

# dataframe untuk analisis model dataset `day` target `casual`
models_day_cas = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'AdaBoost', 'XGBoost'])

# dataframe untuk analisis model dataset `day` target `registered`
models_day_reg = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'AdaBoost', 'XGBoost'])

# dataframe untuk analisis model dataset `hour` target `casual`
models_hour_cas = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'AdaBoost', 'XGBoost'])

# dataframe untuk analisis model dataset `hour` target `registered`
models_hour_reg = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'AdaBoost', 'XGBoost'])

"""### K-Nearest Neighbor"""

# model KNN untuk dataset `day` target `casual`
knn_day_cas = KNeighborsRegressor(n_neighbors=10)
knn_day_cas.fit(X_train_day_c, y_train_day_c)

models_day_cas.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn_day_cas.predict(X_train_day_c), y_true=y_train_day_c)

# model KNN untuk dataset `day` target `registered`
knn_day_reg = KNeighborsRegressor(n_neighbors=10)
knn_day_reg.fit(X_train_day_r, y_train_day_r)

models_day_reg.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn_day_reg.predict(X_train_day_r), y_true=y_train_day_r)

# model KNN untuk dataset `hour` target `casual`
knn_hour_cas = KNeighborsRegressor(n_neighbors=10)
knn_hour_cas.fit(X_train_hour_c, y_train_hour_c)

models_hour_cas.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn_hour_cas.predict(X_train_hour_c), y_true=y_train_hour_c)

# model KNN untuk dataset `hour` target `registered`
knn_hour_reg = KNeighborsRegressor(n_neighbors=10)
knn_hour_reg.fit(X_train_hour_r, y_train_hour_r)

models_hour_reg.loc['train_mse','KNN'] = mean_squared_error(y_pred = knn_hour_reg.predict(X_train_hour_r), y_true=y_train_hour_r)

"""### Random Forest"""

# model RandomForest untuk dataset `day` target `casual`
RF_day_cas = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF_day_cas.fit(X_train_day_c, y_train_day_c)

models_day_cas.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF_day_cas.predict(X_train_day_c), y_true=y_train_day_c)

# model RandomForest untuk dataset `day` target `registered`
RF_day_reg = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF_day_reg.fit(X_train_day_r, y_train_day_r)

models_day_reg.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF_day_reg.predict(X_train_day_r), y_true=y_train_day_r)

# model RandomForest untuk dataset `hour` target `casual`
RF_hour_cas = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF_hour_cas.fit(X_train_hour_c, y_train_hour_c)

models_hour_cas.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF_hour_cas.predict(X_train_hour_c), y_true=y_train_hour_c)

# model RandomForest untuk dataset `hour` target `registered`
RF_hour_reg = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF_hour_reg.fit(X_train_hour_r, y_train_hour_r)

models_hour_reg.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF_hour_reg.predict(X_train_hour_r), y_true=y_train_hour_r)

"""### Adaptive Boosting"""

# model AdaBoost untuk dataset `day` target `casual`
adaboost_day_cas = AdaBoostRegressor(learning_rate=0.05, random_state=55)
adaboost_day_cas.fit(X_train_day_c, y_train_day_c)

models_day_cas.loc['train_mse','AdaBoost'] = mean_squared_error(y_pred=adaboost_day_cas.predict(X_train_day_c), y_true=y_train_day_c)

# model AdaBoost untuk dataset `day` target `registered`
adaboost_day_reg = AdaBoostRegressor(learning_rate=0.05, random_state=55)
adaboost_day_reg.fit(X_train_day_r, y_train_day_r)

models_day_reg.loc['train_mse','AdaBoost'] = mean_squared_error(y_pred=adaboost_day_reg.predict(X_train_day_r), y_true=y_train_day_r)

# model AdaBoost untuk dataset `hour` target `casual`
adaboost_hour_cas = AdaBoostRegressor(learning_rate=0.05, random_state=55)
adaboost_hour_cas.fit(X_train_hour_c, y_train_hour_c)

models_hour_cas.loc['train_mse','AdaBoost'] = mean_squared_error(y_pred=adaboost_hour_cas.predict(X_train_hour_c), y_true=y_train_hour_c)

# model AdaBoost untuk dataset `hour` target `registered`
adaboost_hour_reg = AdaBoostRegressor(learning_rate=0.05, random_state=55)
adaboost_hour_reg.fit(X_train_hour_r, y_train_hour_r)

models_hour_reg.loc['train_mse','AdaBoost'] = mean_squared_error(y_pred=adaboost_hour_reg.predict(X_train_hour_r), y_true=y_train_hour_r)

"""### Gradient Boosting"""

# model XGBoost untuk dataset `day` target `casual`
xgboost_day_cas = XGBRegressor(learning_rate=0.05, random_state=55)
xgboost_day_cas.fit(X_train_day_c, y_train_day_c)

models_day_cas.loc['train_mse','XGBoost'] = mean_squared_error(y_pred=xgboost_day_cas.predict(X_train_day_c), y_true=y_train_day_c)

# model XGBoost untuk dataset `day` target `registered`
xgboost_day_reg = XGBRegressor(learning_rate=0.05, random_state=55)
xgboost_day_reg.fit(X_train_day_r, y_train_day_r)

models_day_reg.loc['train_mse','XGBoost'] = mean_squared_error(y_pred=xgboost_day_reg.predict(X_train_day_r), y_true=y_train_day_r)

# model XGBoost untuk dataset `hour` target `casual`
xgboost_hour_cas = XGBRegressor(learning_rate=0.05, random_state=55)
xgboost_hour_cas.fit(X_train_hour_c, y_train_hour_c)

models_hour_cas.loc['train_mse','XGBoost'] = mean_squared_error(y_pred=xgboost_hour_cas.predict(X_train_hour_c), y_true=y_train_hour_c)

# model XGBoost untuk dataset `hour` target `registered`
xgboost_hour_reg = XGBRegressor(learning_rate=0.05, random_state=55)
xgboost_hour_reg.fit(X_train_hour_r, y_train_hour_r)

models_hour_reg.loc['train_mse','XGBoost'] = mean_squared_error(y_pred=xgboost_hour_reg.predict(X_train_hour_r), y_true=y_train_hour_r)

"""## Evaluasi Model

Evaluasi model regresi akan menggunakan metrik Mean Squared Error (MSE) untuk menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi.
"""

# Lakukan scaling terhadap fitur numerik pada data uji sehingga memiliki skala data yang sama dengan data latih
X_test_day_c.loc[:, numerical_features] = scaler_day_cas.transform(X_test_day_c[numerical_features])
X_test_day_r.loc[:, numerical_features] = scaler_day_reg.transform(X_test_day_r[numerical_features])

X_test_hour_c.loc[:, numerical_features] = scaler_hour_cas.transform(X_test_hour_c[numerical_features])
X_test_hour_r.loc[:, numerical_features] = scaler_hour_reg.transform(X_test_hour_r[numerical_features])

"""### Evaluasi Prediksi Model Dataset `day`

#### Target `casual`
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse_day_cas = pd.DataFrame(columns=['train', 'test'], index=['KNN','RandomForest','AdaBoost','XGBoost'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict_day_c = {'KNN': knn_day_cas, 'RandomForest': RF_day_cas, 'AdaBoost': adaboost_day_cas, 'XGBoost': xgboost_day_cas}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict_day_c.items():
    mse_day_cas.loc[name, 'train'] = mean_squared_error(y_true=y_train_day_c, y_pred=model.predict(X_train_day_c))/1e3
    mse_day_cas.loc[name, 'test'] = mean_squared_error(y_true=y_test_day_c, y_pred=model.predict(X_test_day_c))/1e3

# Panggil mse
mse_day_cas

fig, ax = plt.subplots()
mse_day_cas.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dapat dilihat bahwa untuk dataset `day` dengan target `casual`, model XGBoost memberikan nilai error (MSE) yang paling kecil daripada algoritma lain pada data latih dan data uji, sehingga model XGBoost akan digunakan untuk melakukan prediksi jumlah pengguna `casual` pada dataset `day`."""

prediksi_day_c = X_test_day_c.iloc[:5].copy()
pred_dict_day_c = {'y_true':y_test_day_c[:5]}
for name, model in model_dict_day_c.items():
    pred_dict_day_c['prediksi_'+name] = model.predict(prediksi_day_c).round(0)

pd.DataFrame(pred_dict_day_c)

"""#### Target `registered`"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse_day_reg = pd.DataFrame(columns=['train', 'test'], index=['KNN','RandomForest','AdaBoost','XGBoost'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict_day_r = {'KNN': knn_day_reg, 'RandomForest': RF_day_reg, 'AdaBoost': adaboost_day_reg, 'XGBoost': xgboost_day_reg}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict_day_r.items():
    mse_day_reg.loc[name, 'train'] = mean_squared_error(y_true=y_train_day_r, y_pred=model.predict(X_train_day_r))/1e3
    mse_day_reg.loc[name, 'test'] = mean_squared_error(y_true=y_test_day_r, y_pred=model.predict(X_test_day_r))/1e3

# Panggil mse
mse_day_reg

fig, ax = plt.subplots()
mse_day_reg.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dapat dilihat bahwa untuk dataset `day` dengan target `registered`, model XGBoost memberikan nilai error (MSE) yang paling kecil daripada algoritma lain pada data latih. Akan tetapi, model RandomForest memberikan nilai error (MSE) yang paling kecil daripada algoritma lain pada data uji, sehingga model RandomForest akan digunakan untuk melakukan prediksi jumlah pengguna `registered` pada dataset `day`."""

prediksi_day_r = X_test_day_r.iloc[:5].copy()
pred_dict_day_r = {'y_true':y_test_day_r[:5]}
for name, model in model_dict_day_r.items():
    pred_dict_day_r['prediksi_'+name] = model.predict(prediksi_day_r).round(0)

pd.DataFrame(pred_dict_day_r)

"""### Evaluasi Prediksi Model Dataset `hour`

#### Target `casual`
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse_hour_cas = pd.DataFrame(columns=['train', 'test'], index=['KNN','RandomForest','AdaBoost','XGBoost'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict_hour_c = {'KNN': knn_hour_cas, 'RandomForest': RF_hour_cas, 'AdaBoost': adaboost_hour_cas, 'XGBoost': xgboost_hour_cas}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict_hour_c.items():
    mse_hour_cas.loc[name, 'train'] = mean_squared_error(y_true=y_train_hour_c, y_pred=model.predict(X_train_hour_c))/1e3
    mse_hour_cas.loc[name, 'test'] = mean_squared_error(y_true=y_test_hour_c, y_pred=model.predict(X_test_hour_c))/1e3

# Panggil mse
mse_hour_cas

fig, ax = plt.subplots()
mse_hour_cas.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dapat dilihat bahwa untuk dataset `hour` dengan target `casual`, model RandomForest memberikan nilai error (MSE) yang paling kecil daripada algoritma lain pada data latih dan data uji, sehingga model RandomForest akan digunakan untuk melakukan prediksi jumlah pengguna `casual` pada dataset `hour`."""

prediksi_hour_c = X_test_hour_c.iloc[:5].copy()
pred_dict_hour_c = {'y_true':y_test_hour_c[:5]}
for name, model in model_dict_hour_c.items():
    pred_dict_hour_c['prediksi_'+name] = model.predict(prediksi_hour_c).round(0)

pd.DataFrame(pred_dict_hour_c)

"""#### Target `registered`"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse_hour_reg = pd.DataFrame(columns=['train', 'test'], index=['KNN','RandomForest','AdaBoost','XGBoost'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict_hour_r = {'KNN': knn_hour_reg, 'RandomForest': RF_hour_reg, 'AdaBoost': adaboost_hour_reg, 'XGBoost': xgboost_hour_reg}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict_hour_r.items():
    mse_hour_reg.loc[name, 'train'] = mean_squared_error(y_true=y_train_hour_r, y_pred=model.predict(X_train_hour_r))/1e3
    mse_hour_reg.loc[name, 'test'] = mean_squared_error(y_true=y_test_hour_r, y_pred=model.predict(X_test_hour_r))/1e3

# Panggil mse
mse_hour_reg

fig, ax = plt.subplots()
mse_hour_reg.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dapat dilihat bahwa untuk dataset `hour` dengan target `registered`, model RandomForest memberikan nilai error (MSE) yang paling kecil daripada algoritma lain pada data latih dan data uji, sehingga model RandomForest akan digunakan untuk melakukan prediksi jumlah pengguna `registered` pada dataset `hour`."""

prediksi_day_r = X_test_day_r.iloc[:5].copy()
pred_dict_day_r = {'y_true':y_test_day_r[:5]}
for name, model in model_dict_day_r.items():
    pred_dict_day_r['prediksi_'+name] = model.predict(prediksi_day_r).round(0)

pd.DataFrame(pred_dict_day_r)

"""## Kesimpulan

Dari keempat kasus, yaitu kombinasi dataset `day` dan `hour` dengan kolom target `casual` dan `registered`, model RandomForest memberikan hasil yang paling baik pada 3 dari 4 kasus uji.
"""